{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPIPm2r/XM/l4EHOlR98X8Y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import os\n","import numpy as np\n","import pandas as pd\n","import math \n","import csv\n","import sys\n","\n","from sklearn.metrics import mean_squared_error\n","import matplotlib.pyplot as plt\n","\n","import tensorflow as tf\n","from keras.layers import Input, Dense, Lambda, Layer, Activation\n","from keras.layers.normalization import BatchNormalization\n","from keras.models import Model\n","from keras import backend as K\n","from keras import metrics, optimizers\n","from keras.callbacks import Callback\n","import keras\n"],"metadata":{"id":"5mmnNsVGCQj5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Reparameterization trick \n","def sampling(args):\n","    \n","    z_mean, z_log_var = args\n","\n","    epsilon = K.random_normal(shape=K.shape(z_mean), mean=0., stddev=1.0)\n","    \n","    z = z_mean + K.exp(z_log_var / 2) * epsilon\n","    return z\n","\n","#Vae loss defined\n","def vae_loss(x_input, x_decoded):\n","    reconstruction_loss = original_dim * metrics.mse(x_input, x_decoded)\n","    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n","    print K.get_value(beta)\n","    return K.mean(reconstruction_loss + (K.get_value(beta) * kl_loss))\n","\n","#Reconstruction loss defined\n","def reconstruction_loss(x_input, x_decoded):\n","    return metrics.mse(x_input, x_decoded)\n","\n","#KL loss defined\n","def kl_loss(x_input, x_decoded):\n","    return - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)"],"metadata":{"id":"x2gVLdkHCbaJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pSqeTaldCKwZ"},"outputs":[],"source":["\n","class WarmUpCallback(Callback):\n","    def __init__(self, beta, kappa):\n","        self.beta = beta\n","        self.kappa = kappa\n","    \n","    # Behavior on each epoch\n","    def on_epoch_end(self, epoch, logs={}):\n","        if K.get_value(self.beta) <= 1:\n","            K.set_value(self.beta, K.get_value(self.beta) + self.kappa)\n","\n","#Read input file\n","input_filename = sys.argv[1]\n","output_filename = sys.argv[2]\n","input_df = pd.read_table(input_filename, index_col=0)\n","print \"INPUT FILE...\"\n","print input_df.shape \n","print input_df.head(5)\n","\n","# Set hyperparameters\n","original_dim = input_df.shape[1]\n","intermediate1_dim = int(sys.argv[3])\n","intermediate2_dim = int(sys.argv[4])\n","latent_dim = int(sys.argv[5])\n","\n","batch_size = 50\n","learning_rate = 0.0005\n","beta = K.variable(1)\n","kappa = 0\n","\n","test_data_size = int(sys.argv[6])\n","epochs = int(sys.argv[7])\n","fold_count = int(sys.argv[8])\n","\n","#Separate data to training and test sets\n","input_df_training = input_df.iloc[:-1 * test_data_size, :]\n","input_df_test = input_df.iloc[-1 * test_data_size:, :]\n","\n","print \"INPUT DF\"\n","print input_df_training.shape\n","print input_df_training.index\n","print \"TEST DF\"\n","print input_df_test.shape\n","print input_df_test.index\n","\n","#Define encoder\n","x = Input(shape=(original_dim, ))\n","\n","net = Dense(intermediate1_dim)(x)\n","net2 = BatchNormalization()(net)\n","net3 = Activation('relu')(net2)\n","\n","net4 = Dense(intermediate2_dim)(net3)\n","net5 = BatchNormalization()(net4)\n","net6 = Activation('relu')(net5)\n","\n","z_mean = Dense(latent_dim)(net6)\n","z_log_var = Dense(latent_dim)(net6)\n","\n","# Sample from mean and var\n","z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n","\n","#Define decoder\n","decoder_h = Dense(intermediate2_dim, activation='relu')\n","decoder_h2 = Dense(intermediate1_dim, activation='relu')\n","decoder_mean = Dense(original_dim)\n","\n","h_decoded = decoder_h(z)\n","h_decoded2 = decoder_h2(h_decoded)\n","x_decoded_mean = decoder_mean(h_decoded2)\n","\n","#VAE model\n","vae = Model(x, x_decoded_mean)\n","\n","adam = optimizers.Adam(lr=learning_rate)\n","vae.compile(optimizer=adam, loss = vae_loss, metrics = [reconstruction_loss, kl_loss])\n","vae.summary()\n","\n","#Train from only training data\n","history  = vae.fit(np.array(input_df_training), np.array(input_df_training),\n","               shuffle=True,\n","               epochs=epochs,\n","               batch_size=batch_size,\n","               verbose = 2,\n","               validation_data=(np.array(input_df_test), np.array(input_df_test)),\n","               callbacks=[WarmUpCallback(beta, kappa)])\n","\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('VAE Model Loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='upper left')\n","fig = plt.gcf()\n","fig.set_size_inches(14.5, 8.5)\n","plt.show()\n","\n","plt.plot(history.history['reconstruction_loss'])\n","plt.plot(history.history['val_reconstruction_loss'])\n","plt.title('VAE Model Reconstruction Error')\n","plt.ylabel('reconstruction error')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='upper left')\n","fig = plt.gcf()\n","fig.set_size_inches(14.5, 8.5)\n","plt.show()\n","\n","\n"]},{"cell_type":"code","source":["# DEFINE ENCODER\n","encoder = Model(x, z_mean)\n","\n","#SAVE THE ENCODER\n","from keras.models import model_from_json\n","\n","model_json = encoder.to_json()\n","with open(\"encoder\" + str(fold_count) + \".json\", \"w\") as json_file:\n","    json_file.write(model_json)\n","\n","encoder.save_weights(\"encoder\" + str(fold_count) + \".h5\")\n","print(\"Saved model to disk\")\n","\n","\n","#DEFINE DECODER\n","decoder_input = Input(shape=(latent_dim, )) \n","_h_decoded = decoder_h(decoder_input)\n","_h_decoded2 = decoder_h2(_h_decoded)\n","_x_decoded_mean = decoder_mean(_h_decoded2)\n","decoder = Model(decoder_input, _x_decoded_mean)\n","\n","# Encode test data into the latent representation - and save output\n","test_encoded = encoder.predict(input_df_test, batch_size = batch_size)\n","test_encoded_df = pd.DataFrame(test_encoded, index = input_df_test.index)\n","test_encoded_df.to_csv(output_filename + str(fold_count) + \".tsv\", sep='\\t', quoting = csv.QUOTE_NONE)\n","\n","# How well does the model reconstruct the input data\n","test_reconstructed = decoder.predict(np.array(test_encoded_df))\n","test_reconstructed_df = pd.DataFrame(test_reconstructed, index = input_df_test.index, columns = input_df_test.columns)\n","\n","recons_error = mean_squared_error(np.array(input_df_test), np.array(test_reconstructed_df))\n","\n","print(\"TEST RECONSTRUCTION ERROR: \" + str(recons_error))"],"metadata":{"id":"67BYWk9eCfwh"},"execution_count":null,"outputs":[]}]}